---
title: "Bayesian Statistics"
subtitle: "Notes and Applications"
author:  
date: "Fall 2020"
output:
  html_document:
    number_sections: false
    toc: false
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---



<style type="text/css">


h1.title { /* Title */
    font-size: 30px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h3.subtitle { /* Subtitle */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.author { /* Author */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.date { /* Date */
    font-size: 14px;
    color: DarkBlue;
    font-weight: normal;
    font-style: italic;
}


h1 { /* Header 1 */
    font-size: 20px;
    color: Blue;
    font-weight: normal; <!-- this could be:  font-weight: bold -->
    font-style: normal;
}
h2 { /* Header 2 */
    font-size: 18px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h3 { /* Header 3 */
    font-size: 16px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h4 { /* Header 4 */
    font-size: 14px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h5 { /* Header 5 */
    font-size: 12px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}


body{ /* Normal  */
      font-size: 12px;
      font-family: "Verdana", Times, serif;
  }
td {  /* Table  */
  font-size: 8px;
}


code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>
<div id="examples-from-bayesian-stats-course-columbia-fall-2020" class="section level1">
<h1>Examples from Bayesian Stats Course (Columbia, Fall 2020)</h1>
<div id="example-1a-bayes-rule" class="section level2">
<h2>Example 1a: Bayes Rule</h2>
<p>You go to the doctor for a COVID-19 rapid test. Let <code>theta</code> represent your true state: <code>theta</code> = 1 if you are infected with the virus, and <code>theta</code> = 0 if you are not. Let <code>y</code> denote the result of the test: <code>y</code> = 1 if the test comes back positive and <code>y</code> = 0 if the test is negative. The probability of a correct result for an infected subject is called the sensitivity of the test (<code>Pr(y=1|theta=1)</code>), we will denote it <code>p</code> here. The probability of correct result for one not infected with the virus is called the specificity (<code>Pr(Y=0|theta=0)</code>), denoted as <code>q.</code></p>
<p>Do you have the virus? Of course you don’t know. In Bayesian statistics we describe our uncertainty about the world by assigning probabilities to various states. Suppose your prior probability
of being infected is <code>pi</code>. That is, the marginal probability <code>Pr(theta = 1) = pi</code> .</p>
<p>With this information we can define the following:</p>
<ul>
<li><code>pi</code> as the prior prob of infection (“prevalence”): <code>pi = Pr(theta=1)</code></li>
<li><code>q0</code> is the false positive probability: <code>q0 = Pr(Y=1|theta=0)</code></li>
<li><code>q1</code> is the true positive probability: <code>q1 = Pr(Y=1|theta=1)</code>. This is also called the sensitivity of the test; <code>1-q0</code> is the specificity of the test.</li>
</ul>
<p>We set the following values in R:</p>
<pre class="r"><code>pi &lt;- .04   # 20% of population is infected
q1 &lt;- .90  # test has sensitivity of .90
q0 &lt;- .08  # test has specificity of .92</code></pre>
<p>With this we can compute the posterior probabilities, given test result.</p>
<p>The probability of infection, before the test result is available is <code>pi</code>:</p>
<pre class="r"><code>pi  # Prob of infection, before test result is available </code></pre>
<pre><code>## [1] 0.04</code></pre>
<p>The probability of infection given a positive test is:
<span class="math display">\[Pr(\theta|y_1 =1) = \frac{Pr(\theta = 1, y_1 = 1)}{Pr(y_1=1)} = \frac{Pr(\theta=1)Pr(y_1=1|\theta=1)}{Pr(y_1=1)} = \frac{\pi p }{\pi p + (1-\pi)q}\]</span></p>
<pre class="r"><code>pi * q1 / (pi*q1 + (1-pi)*q0)  # Prob of infection given positive test result</code></pre>
<pre><code>## [1] 0.3191489</code></pre>
<p>The probability of infection given a negative test is:
<span class="math display">\[Pr(\theta|y_1 =0) = \frac{Pr(\theta = 1, y_1 = 1)}{Pr(y_1=0)} = \frac{Pr(\theta=1)Pr(y_1=0|\theta=1)}{Pr(y_1=0)} = \frac{\pi (1-p) }{\pi (1-p) + (1-\pi)(1-q)}\]</span></p>
<pre class="r"><code>pi*(1-q1) / (pi*(1-q1) + (1-pi)*(1-q0))  # Prob of infection if test negative</code></pre>
<pre><code>## [1] 0.004508566</code></pre>
<p>Plot posterior probability versus prior probability:</p>
<pre class="r"><code>post.prob &lt;- function(pi, q0, q1)
{
  pi * q1 / (pi*q1 + (1-pi)*q0)
}

grid &lt;- seq(.01, .99, .01)

plot(grid, post.prob(pi=grid, q0=q0, q1=q1), type=&quot;l&quot;, 
     xlab=&quot;Prior probability&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>That’s how the posterior prob varies with prior prob</p>
<p>What about the sensitivity and specificity of the test?</p>
<pre class="r"><code>op &lt;- par(mfrow=c(1,2))

plot(grid, post.prob(pi=pi, q1=grid, q0=q0), type=&quot;l&quot;, 
     xlab=&quot;Sensitivity of test&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))

plot(grid, post.prob(pi=pi, q1=q1, q0=1-grid), type=&quot;l&quot;,
     xlab=&quot;Specificity of test&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>par(op)</code></pre>
<div id="monte-carlo-approximation" class="section level3">
<h3>Monte Carlo approximation</h3>
<p>Simulate the joint distribution of theta and Y:</p>
<pre class="r"><code>S &lt;- 1000;  theta &lt;- NULL;  y &lt;- NULL;

for(s in 1:S)
{
  theta[s] &lt;- rbinom(1, 1, pi)
  prob &lt;- ifelse(theta[s]==1, q1, q0)
  y[s] &lt;- rbinom(1, 1, prob)
}

table(theta, y) / S  # approximate joint distribution</code></pre>
<pre><code>##      y
## theta     0     1
##     0 0.886 0.077
##     1 0.004 0.033</code></pre>
<pre class="r"><code>mean(theta[y==1])  #  Monte Carlo approx to posterior prob</code></pre>
<pre><code>## [1] 0.3</code></pre>
<p>Of course there’s no real need for Monte Carlo, when the exact joint distribution is available by straightforward calculation:</p>
<pre class="r"><code>joint &lt;- matrix(c((1-pi)*(1-q0), pi*(1-q1), (1-pi)*q0, pi*q1), 2,2)

rownames(joint) &lt;- c(&quot;theta=0&quot;, &quot;theta=1&quot;)

colnames(joint) &lt;- c(&quot;y=0&quot;, &quot;y=1&quot;)

joint</code></pre>
<pre><code>##            y=0    y=1
## theta=0 0.8832 0.0768
## theta=1 0.0040 0.0360</code></pre>
<pre class="r"><code>joint[2,2] / sum(joint[,2])</code></pre>
<pre><code>## [1] 0.3191489</code></pre>
</div>
</div>
<div id="example-1b-beta-binomial-model" class="section level2">
<h2>Example 1b: Beta-Binomial Model</h2>
<p>Create data:</p>
<pre class="r"><code>rm(list=ls())

n &lt;- 100;  y &lt;- 60;

theta &lt;- seq(0,1,.01)</code></pre>
<p>Hyperparameters:</p>
<pre class="r"><code>a          &lt;- c(1,.5,2,20)
b          &lt;- c(1,.5,2, 1)</code></pre>
<p>Calculations:</p>
<pre class="r"><code>post_mean  &lt;- (y+a)/(n+a+b)
post_var   &lt;- (y+a)*(n-y+b)/((n+a+b)^2 * (n+a+b+1))
post_sd    &lt;- sqrt(post_var)
post_PG5   &lt;- 1-pbeta(0.5,a+y,b+n-y)
pri_mean   &lt;- (a)/(a+b)
pri_var    &lt;- (a)*(b)/((a+b)*(a+b)*(a+b+1))
pri_sd     &lt;- sqrt(pri_var)
pri_PG5    &lt;- 1-pbeta(0.5,a,b)</code></pre>
<p>Summary plot of prior and posterior on same axes</p>
<pre class="r"><code>plot(theta,dbeta(theta,1,1),lwd=2,type=&quot;l&quot;,
     xlab=expression(theta), ylab=&quot;Density&quot;,
     ylim=c(0,10), cex.lab=1.5)  

legend(&quot;topleft&quot;, c(&quot;Prior&quot;,&quot;Posterior&quot;), col=1:2,
       lwd=2, inset=0.05, cex=1.25)

lines(theta, dbeta(theta,y+1,n-y+1), col=2, lwd=2)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Consider four different priors:</p>
<pre class="r"><code>op &lt;- par(mfrow=c(2,2))

for(j in 1:4)
{
  plot(theta,dbeta(theta,a[j],b[j]), lwd=2, type=&quot;l&quot;, ylim=c(0,10), 
       xlab=expression(theta), ylab=&quot;Density&quot;,cex.lab=1.25)  
  legend(&quot;topleft&quot;,c(&quot;Prior&quot;,&quot;Posterior&quot;),col=1:2,lwd=2,inset=0.05)
  lines(theta,dbeta(theta,y+a[j],n-y+b[j]),col=2,lwd=2)
}</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>par(op)</code></pre>
<p>Compare the four priors to each other (ignoring data)</p>
<pre class="r"><code>plot(NA, xlim=0:1, xlab=expression(theta), ylab=&quot;Prior density&quot;,
     ylim=c(0,10), cex.lab=1.25)  

for(j in 1:4)
{
  lines(theta, dbeta(theta, a[j], b[j]), col=j, lwd=2)
}

legend(&quot;topleft&quot;,
       c(&quot;a=1    b=1&quot;,&quot;a=0.5 b=0.5&quot;, &quot;a=2    b=2&quot;,&quot;a=20  b=1&quot;), 
       col=1:4, lwd=2, inset=0.05, cex=1.25)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Compare the four posteriors (single set of axes)</p>
<pre class="r"><code>plot(NA, xlim=0:1, xlab=expression(theta), ylab=&quot;Posterior density&quot;,
     ylim=c(0,10), cex.lab=1.25)  

for(j in 1:4)
{
  lines(theta, dbeta(theta, y+a[j], n-y+b[j]), col=j, lwd=2)
}

legend(&quot;topleft&quot;,
       c(&quot;a=1    b=1&quot;,&quot;a=0.5 b=0.5&quot;, &quot;a=2    b=2&quot;,&quot;a=20  b=1&quot;),
       col=1:4, lwd=2, inset=0.05, cex=1.25)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Numerical summaries</p>
<pre class="r"><code>round(cbind(a, b, pri_mean, pri_sd, pri_PG5, 
            post_mean, post_sd, post_PG5), 2)</code></pre>
<pre><code>##         a   b pri_mean pri_sd pri_PG5 post_mean post_sd post_PG5
## [1,]  1.0 1.0     0.50   0.29     0.5      0.60    0.05     0.98
## [2,]  0.5 0.5     0.50   0.35     0.5      0.60    0.05     0.98
## [3,]  2.0 2.0     0.50   0.22     0.5      0.60    0.05     0.98
## [4,] 20.0 1.0     0.95   0.05     1.0      0.66    0.04     1.00</code></pre>
</div>
<div id="example-1c-summarizing-a-posterior" class="section level2">
<h2>Example 1c: Summarizing a Posterior</h2>
<pre class="r"><code>rm(list=ls()); set.seed(20200909); </code></pre>
<p>Data</p>
<pre class="r"><code>y &lt;- 8; n &lt;- 10</code></pre>
<p>The posterior is theta|y~Beta(A,B)</p>
<pre class="r"><code>A &lt;- y+1; B &lt;- n-y+1</code></pre>
<p>Posterior mean</p>
<pre class="r"><code>A/(A+B)</code></pre>
<pre><code>## [1] 0.75</code></pre>
<p>Posterior standard deviation</p>
<pre class="r"><code>sqrt(A*B/((A+B)*(A+B)*(A+B+1)))</code></pre>
<pre><code>## [1] 0.1200961</code></pre>
<p>Posterior 95% credible interval</p>
<pre class="r"><code>qbeta(c(0.025,0.975),A,B)</code></pre>
<pre><code>## [1] 0.4822441 0.9397823</code></pre>
<p>Posterior probability that theta&lt;0.5</p>
<pre class="r"><code>pbeta(0.5,A,B)</code></pre>
<pre><code>## [1] 0.03271484</code></pre>
<div id="inference-about-odds" class="section level3">
<h3>Inference about odds</h3>
<p>If theta is a prob the odds are gamma = theta/(1-theta)</p>
<pre class="r"><code>theta &lt;- rbeta(1000, A, B)</code></pre>
<p>Approximate the posterior mean and SD using Monte Carlo</p>
<pre class="r"><code>mean(theta); sd(theta);</code></pre>
<pre><code>## [1] 0.7481332</code></pre>
<pre><code>## [1] 0.1162733</code></pre>
<p>Transform to odds</p>
<pre class="r"><code>gamma &lt;- theta/(1-theta)</code></pre>
<p>Approximate the posterior mean and SD</p>
<pre class="r"><code>mean(gamma); sd(gamma);</code></pre>
<pre><code>## [1] 4.56484</code></pre>
<pre><code>## [1] 6.254285</code></pre>
<p>Show posterior density, mark mean, median and mode</p>
<pre class="r"><code>vline &lt;- function(x, A, B, lwd=1, col=1, lty=1)
{
  theta &lt;- seq(0, 1, length=1000)
  k     &lt;- theta[which.min(abs(theta-x))]
  lines(c(k,k), c(0,dbeta(k,A,B)), lwd=lwd, col=col, lty=lty) 
}

MAP &lt;- (A-1)/(A+B-2)     # Posterior mode

MN  &lt;- A/(A+B)           # Posterior mean

MD  &lt;- qbeta(0.5, A, B)  # Posterior median

theta &lt;- seq(0,1,length=100)

plot(theta,dbeta(theta,A,B), lwd=2, type=&quot;l&quot;,
     xlab=expression(theta), ylab=&quot;Posterior density&quot;)

vline(MN,  A, B, lwd=2, col=1)

vline(MAP, A, B, lwd=2, col=2)

vline(MD,  A, B, lwd=2, col=3)

legend(&quot;topleft&quot;,c(&quot;Post mean&quot;,&quot;MAP&quot;,&quot;Post median&quot;), 
       col=1:3, lwd=2, bty=&quot;n&quot;, cex=1.25)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
<div id="bayesian-credible-intervals" class="section level3">
<h3>Bayesian credible intervals</h3>
<pre class="r"><code>plot(theta, dbeta(theta,A,B), type=&quot;l&quot;, lwd=2, 
     xlab=expression(theta), ylab=&quot;Posterior density&quot;)

vline(qbeta(0.1,  A,B), A, B, lwd=2, col=1)

vline(qbeta(0.9,  A,B), A, B, lwd=2, col=1)

vline(qbeta(0.05, A,B), A, B, lwd=2, col=2)

vline(qbeta(0.95, A,B), A, B, lwd=2, col=2)

vline(qbeta(0.025,A,B), A, B, lwd=2, col=3)

vline(qbeta(0.975,A,B), A, B, lwd=2, col=3)

legend(&quot;topleft&quot;,
       c(&quot;80% cred set&quot;,&quot;90% cred set&quot;,&quot;95% cred set&quot;),
       col=1:3, lwd=2, bty=&quot;n&quot;, cex=1.25)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="datacamp-bayesian-data-analysis-course" class="section level1">
<h1>DataCamp Bayesian Data Analysis Course</h1>
<div id="chapter-1" class="section level2">
<h2>Chapter 1</h2>
<div id="a-model-for-a-zombie-drug" class="section level3">
<h3>A Model for a Zombie Drug</h3>
<p>The role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update probability distributions to reflect what has ben learned from data.</p>
<p>A model for the proportion of success (success can be clicking and add, or curing a patient):</p>
<ul>
<li>The data is a vector of successes and failures represented by 0 an 1.</li>
<li>There is an unknown underlying <em>proportion of success</em>.</li>
<li>If a data point is a success, this is only a result of this underlying proportion.</li>
<li>Prior to seeing any data, any underlying proportion of success is equally likely.</li>
<li>The result is a probability distribution that represents what the model knows about the underlying proportion of success.</li>
</ul>
<p>The model, <code>prop_model</code>, can be written in R as follows (this was written by DataCamp):</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ ggplot2 3.2.1     ✓ purrr   0.3.3
## ✓ tibble  2.1.3     ✓ dplyr   0.8.4
## ✓ tidyr   1.0.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(ggjoy)</code></pre>
<pre><code>## Loading required package: ggridges</code></pre>
<pre><code>## The ggjoy package has been deprecated. Please switch over to the
## ggridges package, which provides the same functionality. Porting
## guidelines can be found here:
## https://github.com/clauswilke/ggjoy/blob/master/README.md</code></pre>
<pre class="r"><code>prop_model &lt;- function (data = c(), prior_prop = c(1, 1), n_draws = 10000, 
    show_plot = TRUE) 
{
    data &lt;- as.logical(data)
    proportion_success &lt;- c(0, seq(0, 1, length.out = 100), 1)
    data_indices &lt;- round(seq(0, length(data), length.out = min(length(data) + 
        1, 20)))
    post_curves &lt;- map_dfr(data_indices, function(i) {
        value &lt;- ifelse(i == 0, &quot;Prior&quot;, ifelse(data[i], &quot;Success&quot;, 
            &quot;Failure&quot;))
        label &lt;- paste0(&quot;n=&quot;, i)
        probability &lt;- dbeta(proportion_success, prior_prop[1] + 
            sum(data[seq_len(i)]), prior_prop[2] + sum(!data[seq_len(i)]))
        probability &lt;- probability/max(probability)
        tibble(value, label, proportion_success, probability)
    })
    post_curves$label &lt;- fct_rev(factor(post_curves$label, levels = paste0(&quot;n=&quot;, 
        data_indices)))
    post_curves$value &lt;- factor(post_curves$value, levels = c(&quot;Prior&quot;, 
        &quot;Success&quot;, &quot;Failure&quot;))
    p &lt;- ggplot(post_curves, aes(x = proportion_success, y = label, 
        height = probability, fill = value)) + geom_joy(stat = &quot;identity&quot;, 
        color = &quot;white&quot;, alpha = 0.8, panel_scaling = TRUE, size = 1) + 
        scale_y_discrete(&quot;&quot;, expand = c(0.01, 0)) + scale_x_continuous(&quot;Underlying proportion of success&quot;) + 
        scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), 
            name = &quot;&quot;, drop = FALSE, labels = c(&quot;Prior   &quot;, &quot;Success   &quot;, 
                &quot;Failure   &quot;)) + theme_light(base_size = 18) + 
        theme(legend.position = &quot;top&quot;)
    if (show_plot) {
        print(p)
    }
    invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + 
        sum(!data)))
}</code></pre>
<p>Now let’s try and see. Assume you just flipped a coin four times and the result was heads, tails, tails, heads. If you code heads as a success and tails as a failure then the following R codes runs prop_model with this data</p>
<pre class="r"><code>data &lt;- c(1, 0, 0, 1)
prop_model(data)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>The output of <code>prop_model</code> is a plot showing what the model learns about the underlying proportion of success from each data point in the order you entered them. At n=0 there is no data, and all the model knows is that it’s equally probable that the proportion of success is anything from 0% to 100%. At n=4 all data has been added, and the model knows a little bit more.</p>
<p>If we really were interested in the underlying proportion of heads of this coin then <code>prop_model</code> isn’t particularly useful. Since it assumes that any underlying proportion of success is equally likely prior to seeing any data it will take a lot of coin flipping to convince <code>prop_model</code> that the coin is fair. This model is more appropriate in a situation where we have little background knowledge about the underlying proportion of success.</p>
<p>Let’s say the zombie apocalypse is upon us and we have come up with a new experimental drug to cure zombieism. We have no clue how effective it’s going to be, but when we gave it to 13 zombies two of them turned human again. Change the data argument to <code>prop_model</code> to estimate the underlying proportion of success of curing a zombie.</p>
<pre class="r"><code>data &lt;- c(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
prop_model(data)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-32-1.png" width="672" />
The model implemented in prop_model makes more sense here as we had no clue how good the drug would be. The final probability distribution (at n=13) represents what the model now knows about the underlying proportion of cured zombies. What proportion of zombies would we expect to turn human if we administered this new drug to the whole zombie population?</p>
<p>Answer: Between 5% to 40%.</p>
<div id="priors-and-posteriors" class="section level4">
<h4>Priors and Posteriors</h4>
<p>Here again is the <code>prop_model</code> function which has been given the data from our zombie experiment where two out of 13 zombies got cured. In addition to producing a plot, <code>prop_model</code> also returns a large random sample from the posterior over the underlying proportion of success.</p>
<p>Assign the return value of <code>prop_model</code> to a variable called <code>posterior</code> and take a look at the first number of samples using the command <code>head(posterior)</code></p>
<pre class="r"><code>data = c(1, 0, 0, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0)
posterior &lt;- prop_model(data)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>head(posterior)</code></pre>
<pre><code>## [1] 0.28618601 0.52663955 0.29492983 0.03172872 0.12646159 0.22545022</code></pre>
<p>Looking at these first few samples confirms what is already shown in the plot: That the underlying proportion of cured zombies is likely somewhere between 5% and 50%. But these were just the first six samples in posterior which currently contain 10,000 samples (the default of <code>prop_model</code>).</p>
<p>Take a look at the distribution of all the samples in posterior by plotting it as a histogram using the <code>hist()</code>function with posterior as the first argument.</p>
<pre class="r"><code>hist(posterior, breaks = 30, xlim = c(0,1), col = &quot;palegreen4&quot;)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Compare this histogram to the plot produced directly by prop_model. You should notice that the histogram and the posterior distribution (at n=13) describe the same distribution.</p>
</div>
<div id="summarizing-the-drug-experiment" class="section level4">
<h4>Summarizing the Drug Experiment</h4>
<p>A point estimate is a single number used to summarize what’s known about a parameter of interest. It can be seen as a “best guess” of the value of the parameter. A commonly used point estimate is the median of the posterior. It’s the midpoint of the distribution, and it’s equally probable for the parameter value to be larger than the median as it is to be smaller than it.</p>
<pre class="r"><code>median(posterior)</code></pre>
<pre><code>## [1] 0.1857518</code></pre>
<p>So, a best guess is that the drug would cure around 18% of all zombies. Another common summary is to report an interval that includes the parameter of interest with a certain probability. This is called a <em>credible interval</em> (CI). With a posterior represented as a vector of samples you can calculate a CI using the <code>quantile()</code> function. Let’s calculate the 90% credible interval of <code>posterior</code></p>
<pre class="r"><code>quantile(posterior,c(.05,.95))</code></pre>
<pre><code>##         5%        95% 
## 0.05990058 0.38679638</code></pre>
<pre class="r"><code>quantile(posterior,c(.05,.95))</code></pre>
<pre><code>##         5%        95% 
## 0.05990058 0.38679638</code></pre>
<style type="text/css">


h1.title { /* Title */
    font-size: 30px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h3.subtitle { /* Subtitle */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.author { /* Author */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.date { /* Date */
    font-size: 14px;
    color: DarkBlue;
    font-weight: normal;
    font-style: italic;
}


h1 { /* Header 1 */
    font-size: 20px;
    color: Blue;
    font-weight: normal; <!-- this could be:  font-weight: bold -->
    font-style: normal;
}
h2 { /* Header 2 */
    font-size: 18px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h3 { /* Header 3 */
    font-size: 16px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h4 { /* Header 4 */
    font-size: 14px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h5 { /* Header 5 */
    font-size: 12px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}


body{ /* Normal  */
      font-size: 12px;
      font-family: "Verdana", Times, serif;
  }
td {  /* Table  */
  font-size: 8px;
}


code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>
</div>
</div>
</div>
</div>
<div id="examples-from-bayesian-stats-course-columbia-fall-2020-1" class="section level1">
<h1>Examples from Bayesian Stats Course (Columbia, Fall 2020)</h1>
<div id="example-1a-bayes-rule-1" class="section level2">
<h2>Example 1a: Bayes Rule</h2>
<p>You go to the doctor for a COVID-19 rapid test. Let <code>theta</code> represent your true state: <code>theta</code> = 1 if you are infected with the virus, and <code>theta</code> = 0 if you are not. Let <code>y</code> denote the result of the test: <code>y</code> = 1 if the test comes back positive and <code>y</code> = 0 if the test is negative. The probability of a correct result for an infected subject is called the sensitivity of the test (<code>Pr(y=1|theta=1)</code>), we will denote it <code>p</code> here. The probability of correct result for one not infected with the virus is called the specificity (<code>Pr(Y=0|theta=0)</code>), denoted as <code>q.</code></p>
<p>Do you have the virus? Of course you don’t know. In Bayesian statistics we describe our uncertainty about the world by assigning probabilities to various states. Suppose your prior probability
of being infected is <code>pi</code>. That is, the marginal probability <code>Pr(theta = 1) = pi</code> .</p>
<p>With this information we can define the following:</p>
<ul>
<li><code>pi</code> as the prior prob of infection (“prevalence”): <code>pi = Pr(theta=1)</code></li>
<li><code>q0</code> is the false positive probability: <code>q0 = Pr(Y=1|theta=0)</code></li>
<li><code>q1</code> is the true positive probability: <code>q1 = Pr(Y=1|theta=1)</code>. This is also called the sensitivity of the test; <code>1-q0</code> is the specificity of the test.</li>
</ul>
<p>We set the following values in R:</p>
<pre class="r"><code>pi &lt;- .04   # 20% of population is infected
q1 &lt;- .90  # test has sensitivity of .90
q0 &lt;- .08  # test has specificity of .92</code></pre>
<p>With this we can compute the posterior probabilities, given test result.</p>
<p>The probability of infection, before the test result is available is <code>pi</code>:</p>
<pre class="r"><code>pi  # Prob of infection, before test result is available </code></pre>
<pre><code>## [1] 0.04</code></pre>
<p>The probability of infection given a positive test is:
<span class="math display">\[Pr(\theta|y_1 =1) = \frac{Pr(\theta = 1, y_1 = 1)}{Pr(y_1=1)} = \frac{Pr(\theta=1)Pr(y_1=1|\theta=1)}{Pr(y_1=1)} = \frac{\pi p }{\pi p + (1-\pi)q}\]</span></p>
<pre class="r"><code>pi * q1 / (pi*q1 + (1-pi)*q0)  # Prob of infection given positive test result</code></pre>
<pre><code>## [1] 0.3191489</code></pre>
<p>The probability of infection given a negative test is:
<span class="math display">\[Pr(\theta|y_1 =0) = \frac{Pr(\theta = 1, y_1 = 1)}{Pr(y_1=0)} = \frac{Pr(\theta=1)Pr(y_1=0|\theta=1)}{Pr(y_1=0)} = \frac{\pi (1-p) }{\pi (1-p) + (1-\pi)(1-q)}\]</span></p>
<pre class="r"><code>pi*(1-q1) / (pi*(1-q1) + (1-pi)*(1-q0))  # Prob of infection if test negative</code></pre>
<pre><code>## [1] 0.004508566</code></pre>
<p>Plot posterior probability versus prior probability:</p>
<pre class="r"><code>post.prob &lt;- function(pi, q0, q1)
{
  pi * q1 / (pi*q1 + (1-pi)*q0)
}

grid &lt;- seq(.01, .99, .01)

plot(grid, post.prob(pi=grid, q0=q0, q1=q1), type=&quot;l&quot;, 
     xlab=&quot;Prior probability&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>That’s how the posterior prob varies with prior prob</p>
<p>What about the sensitivity and specificity of the test?</p>
<pre class="r"><code>op &lt;- par(mfrow=c(1,2))

plot(grid, post.prob(pi=pi, q1=grid, q0=q0), type=&quot;l&quot;, 
     xlab=&quot;Sensitivity of test&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))

plot(grid, post.prob(pi=pi, q1=q1, q0=1-grid), type=&quot;l&quot;,
     xlab=&quot;Specificity of test&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>par(op)</code></pre>
<div id="monte-carlo-approximation-1" class="section level3">
<h3>Monte Carlo approximation</h3>
<p>Simulate the joint distribution of theta and Y:</p>
<pre class="r"><code>S &lt;- 1000;  theta &lt;- NULL;  y &lt;- NULL;

for(s in 1:S)
{
  theta[s] &lt;- rbinom(1, 1, pi)
  prob &lt;- ifelse(theta[s]==1, q1, q0)
  y[s] &lt;- rbinom(1, 1, prob)
}

table(theta, y) / S  # approximate joint distribution</code></pre>
<pre><code>##      y
## theta     0     1
##     0 0.886 0.077
##     1 0.004 0.033</code></pre>
<pre class="r"><code>mean(theta[y==1])  #  Monte Carlo approx to posterior prob</code></pre>
<pre><code>## [1] 0.3</code></pre>
<p>Of course there’s no real need for Monte Carlo, when the exact joint distribution is available by straightforward calculation:</p>
<pre class="r"><code>joint &lt;- matrix(c((1-pi)*(1-q0), pi*(1-q1), (1-pi)*q0, pi*q1), 2,2)

rownames(joint) &lt;- c(&quot;theta=0&quot;, &quot;theta=1&quot;)

colnames(joint) &lt;- c(&quot;y=0&quot;, &quot;y=1&quot;)

joint</code></pre>
<pre><code>##            y=0    y=1
## theta=0 0.8832 0.0768
## theta=1 0.0040 0.0360</code></pre>
<pre class="r"><code>joint[2,2] / sum(joint[,2])</code></pre>
<pre><code>## [1] 0.3191489</code></pre>
</div>
</div>
<div id="example-1b-beta-binomial-model-1" class="section level2">
<h2>Example 1b: Beta-Binomial Model</h2>
<p>Create data:</p>
<pre class="r"><code>rm(list=ls())

n &lt;- 100;  y &lt;- 60;

theta &lt;- seq(0,1,.01)</code></pre>
<p>Hyperparameters:</p>
<pre class="r"><code>a          &lt;- c(1,.5,2,20)
b          &lt;- c(1,.5,2, 1)</code></pre>
<p>Calculations:</p>
<pre class="r"><code>post_mean  &lt;- (y+a)/(n+a+b)
post_var   &lt;- (y+a)*(n-y+b)/((n+a+b)^2 * (n+a+b+1))
post_sd    &lt;- sqrt(post_var)
post_PG5   &lt;- 1-pbeta(0.5,a+y,b+n-y)
pri_mean   &lt;- (a)/(a+b)
pri_var    &lt;- (a)*(b)/((a+b)*(a+b)*(a+b+1))
pri_sd     &lt;- sqrt(pri_var)
pri_PG5    &lt;- 1-pbeta(0.5,a,b)</code></pre>
<p>Summary plot of prior and posterior on same axes</p>
<pre class="r"><code>plot(theta,dbeta(theta,1,1),lwd=2,type=&quot;l&quot;,
     xlab=expression(theta), ylab=&quot;Density&quot;,
     ylim=c(0,10), cex.lab=1.5)  

legend(&quot;topleft&quot;, c(&quot;Prior&quot;,&quot;Posterior&quot;), col=1:2,
       lwd=2, inset=0.05, cex=1.25)

lines(theta, dbeta(theta,y+1,n-y+1), col=2, lwd=2)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Consider four different priors:</p>
<pre class="r"><code>op &lt;- par(mfrow=c(2,2))

for(j in 1:4)
{
  plot(theta,dbeta(theta,a[j],b[j]), lwd=2, type=&quot;l&quot;, ylim=c(0,10), 
       xlab=expression(theta), ylab=&quot;Density&quot;,cex.lab=1.25)  
  legend(&quot;topleft&quot;,c(&quot;Prior&quot;,&quot;Posterior&quot;),col=1:2,lwd=2,inset=0.05)
  lines(theta,dbeta(theta,y+a[j],n-y+b[j]),col=2,lwd=2)
}</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>par(op)</code></pre>
<p>Compare the four priors to each other (ignoring data)</p>
<pre class="r"><code>plot(NA, xlim=0:1, xlab=expression(theta), ylab=&quot;Prior density&quot;,
     ylim=c(0,10), cex.lab=1.25)  

for(j in 1:4)
{
  lines(theta, dbeta(theta, a[j], b[j]), col=j, lwd=2)
}

legend(&quot;topleft&quot;,
       c(&quot;a=1    b=1&quot;,&quot;a=0.5 b=0.5&quot;, &quot;a=2    b=2&quot;,&quot;a=20  b=1&quot;), 
       col=1:4, lwd=2, inset=0.05, cex=1.25)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Compare the four posteriors (single set of axes)</p>
<pre class="r"><code>plot(NA, xlim=0:1, xlab=expression(theta), ylab=&quot;Posterior density&quot;,
     ylim=c(0,10), cex.lab=1.25)  

for(j in 1:4)
{
  lines(theta, dbeta(theta, y+a[j], n-y+b[j]), col=j, lwd=2)
}

legend(&quot;topleft&quot;,
       c(&quot;a=1    b=1&quot;,&quot;a=0.5 b=0.5&quot;, &quot;a=2    b=2&quot;,&quot;a=20  b=1&quot;),
       col=1:4, lwd=2, inset=0.05, cex=1.25)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Numerical summaries</p>
<pre class="r"><code>round(cbind(a, b, pri_mean, pri_sd, pri_PG5, 
            post_mean, post_sd, post_PG5), 2)</code></pre>
<pre><code>##         a   b pri_mean pri_sd pri_PG5 post_mean post_sd post_PG5
## [1,]  1.0 1.0     0.50   0.29     0.5      0.60    0.05     0.98
## [2,]  0.5 0.5     0.50   0.35     0.5      0.60    0.05     0.98
## [3,]  2.0 2.0     0.50   0.22     0.5      0.60    0.05     0.98
## [4,] 20.0 1.0     0.95   0.05     1.0      0.66    0.04     1.00</code></pre>
</div>
<div id="example-1c-summarizing-a-posterior-1" class="section level2">
<h2>Example 1c: Summarizing a Posterior</h2>
<pre class="r"><code>rm(list=ls()); set.seed(20200909); </code></pre>
<p>Data</p>
<pre class="r"><code>y &lt;- 8; n &lt;- 10</code></pre>
<p>The posterior is theta|y~Beta(A,B)</p>
<pre class="r"><code>A &lt;- y+1; B &lt;- n-y+1</code></pre>
<p>Posterior mean</p>
<pre class="r"><code>A/(A+B)</code></pre>
<pre><code>## [1] 0.75</code></pre>
<p>Posterior standard deviation</p>
<pre class="r"><code>sqrt(A*B/((A+B)*(A+B)*(A+B+1)))</code></pre>
<pre><code>## [1] 0.1200961</code></pre>
<p>Posterior 95% credible interval</p>
<pre class="r"><code>qbeta(c(0.025,0.975),A,B)</code></pre>
<pre><code>## [1] 0.4822441 0.9397823</code></pre>
<p>Posterior probability that theta&lt;0.5</p>
<pre class="r"><code>pbeta(0.5,A,B)</code></pre>
<pre><code>## [1] 0.03271484</code></pre>
<div id="inference-about-odds-1" class="section level3">
<h3>Inference about odds</h3>
<p>If theta is a prob the odds are gamma = theta/(1-theta)</p>
<pre class="r"><code>theta &lt;- rbeta(1000, A, B)</code></pre>
<p>Approximate the posterior mean and SD using Monte Carlo</p>
<pre class="r"><code>mean(theta); sd(theta);</code></pre>
<pre><code>## [1] 0.7481332</code></pre>
<pre><code>## [1] 0.1162733</code></pre>
<p>Transform to odds</p>
<pre class="r"><code>gamma &lt;- theta/(1-theta)</code></pre>
<p>Approximate the posterior mean and SD</p>
<pre class="r"><code>mean(gamma); sd(gamma);</code></pre>
<pre><code>## [1] 4.56484</code></pre>
<pre><code>## [1] 6.254285</code></pre>
<p>Show posterior density, mark mean, median and mode</p>
<pre class="r"><code>vline &lt;- function(x, A, B, lwd=1, col=1, lty=1)
{
  theta &lt;- seq(0, 1, length=1000)
  k     &lt;- theta[which.min(abs(theta-x))]
  lines(c(k,k), c(0,dbeta(k,A,B)), lwd=lwd, col=col, lty=lty) 
}

MAP &lt;- (A-1)/(A+B-2)     # Posterior mode

MN  &lt;- A/(A+B)           # Posterior mean

MD  &lt;- qbeta(0.5, A, B)  # Posterior median

theta &lt;- seq(0,1,length=100)

plot(theta,dbeta(theta,A,B), lwd=2, type=&quot;l&quot;,
     xlab=expression(theta), ylab=&quot;Posterior density&quot;)

vline(MN,  A, B, lwd=2, col=1)

vline(MAP, A, B, lwd=2, col=2)

vline(MD,  A, B, lwd=2, col=3)

legend(&quot;topleft&quot;,c(&quot;Post mean&quot;,&quot;MAP&quot;,&quot;Post median&quot;), 
       col=1:3, lwd=2, bty=&quot;n&quot;, cex=1.25)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
<div id="bayesian-credible-intervals-1" class="section level3">
<h3>Bayesian credible intervals</h3>
<pre class="r"><code>plot(theta, dbeta(theta,A,B), type=&quot;l&quot;, lwd=2, 
     xlab=expression(theta), ylab=&quot;Posterior density&quot;)

vline(qbeta(0.1,  A,B), A, B, lwd=2, col=1)

vline(qbeta(0.9,  A,B), A, B, lwd=2, col=1)

vline(qbeta(0.05, A,B), A, B, lwd=2, col=2)

vline(qbeta(0.95, A,B), A, B, lwd=2, col=2)

vline(qbeta(0.025,A,B), A, B, lwd=2, col=3)

vline(qbeta(0.975,A,B), A, B, lwd=2, col=3)

legend(&quot;topleft&quot;,
       c(&quot;80% cred set&quot;,&quot;90% cred set&quot;,&quot;95% cred set&quot;),
       col=1:3, lwd=2, bty=&quot;n&quot;, cex=1.25)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="datacamp-bayesian-data-analysis-course-1" class="section level1">
<h1>DataCamp Bayesian Data Analysis Course</h1>
<div id="chapter-1-1" class="section level2">
<h2>Chapter 1</h2>
<div id="a-model-for-a-zombie-drug-1" class="section level3">
<h3>A Model for a Zombie Drug</h3>
<p>The role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update probability distributions to reflect what has ben learned from data.</p>
<p>A model for the proportion of success (success can be clicking and add, or curing a patient):</p>
<ul>
<li>The data is a vector of successes and failures represented by 0 an 1.</li>
<li>There is an unknown underlying <em>proportion of success</em>.</li>
<li>If a data point is a success, this is only a result of this underlying proportion.</li>
<li>Prior to seeing any data, any underlying proportion of success is equally likely.</li>
<li>The result is a probability distribution that represents what the model knows about the underlying proportion of success.</li>
</ul>
<p>The model, <code>prop_model</code>, can be written in R as follows (this was written by DataCamp):</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ ggplot2 3.2.1     ✓ purrr   0.3.3
## ✓ tibble  2.1.3     ✓ dplyr   0.8.4
## ✓ tidyr   1.0.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(ggjoy)</code></pre>
<pre><code>## Loading required package: ggridges</code></pre>
<pre><code>## The ggjoy package has been deprecated. Please switch over to the
## ggridges package, which provides the same functionality. Porting
## guidelines can be found here:
## https://github.com/clauswilke/ggjoy/blob/master/README.md</code></pre>
<pre class="r"><code>prop_model &lt;- function (data = c(), prior_prop = c(1, 1), n_draws = 10000, 
    show_plot = TRUE) 
{
    data &lt;- as.logical(data)
    proportion_success &lt;- c(0, seq(0, 1, length.out = 100), 1)
    data_indices &lt;- round(seq(0, length(data), length.out = min(length(data) + 
        1, 20)))
    post_curves &lt;- map_dfr(data_indices, function(i) {
        value &lt;- ifelse(i == 0, &quot;Prior&quot;, ifelse(data[i], &quot;Success&quot;, 
            &quot;Failure&quot;))
        label &lt;- paste0(&quot;n=&quot;, i)
        probability &lt;- dbeta(proportion_success, prior_prop[1] + 
            sum(data[seq_len(i)]), prior_prop[2] + sum(!data[seq_len(i)]))
        probability &lt;- probability/max(probability)
        tibble(value, label, proportion_success, probability)
    })
    post_curves$label &lt;- fct_rev(factor(post_curves$label, levels = paste0(&quot;n=&quot;, 
        data_indices)))
    post_curves$value &lt;- factor(post_curves$value, levels = c(&quot;Prior&quot;, 
        &quot;Success&quot;, &quot;Failure&quot;))
    p &lt;- ggplot(post_curves, aes(x = proportion_success, y = label, 
        height = probability, fill = value)) + geom_joy(stat = &quot;identity&quot;, 
        color = &quot;white&quot;, alpha = 0.8, panel_scaling = TRUE, size = 1) + 
        scale_y_discrete(&quot;&quot;, expand = c(0.01, 0)) + scale_x_continuous(&quot;Underlying proportion of success&quot;) + 
        scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), 
            name = &quot;&quot;, drop = FALSE, labels = c(&quot;Prior   &quot;, &quot;Success   &quot;, 
                &quot;Failure   &quot;)) + theme_light(base_size = 18) + 
        theme(legend.position = &quot;top&quot;)
    if (show_plot) {
        print(p)
    }
    invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + 
        sum(!data)))
}</code></pre>
<p>Now let’s try and see. Assume you just flipped a coin four times and the result was heads, tails, tails, heads. If you code heads as a success and tails as a failure then the following R codes runs prop_model with this data</p>
<pre class="r"><code>data &lt;- c(1, 0, 0, 1)
prop_model(data)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>The output of <code>prop_model</code> is a plot showing what the model learns about the underlying proportion of success from each data point in the order you entered them. At n=0 there is no data, and all the model knows is that it’s equally probable that the proportion of success is anything from 0% to 100%. At n=4 all data has been added, and the model knows a little bit more.</p>
<p>If we really were interested in the underlying proportion of heads of this coin then <code>prop_model</code> isn’t particularly useful. Since it assumes that any underlying proportion of success is equally likely prior to seeing any data it will take a lot of coin flipping to convince <code>prop_model</code> that the coin is fair. This model is more appropriate in a situation where we have little background knowledge about the underlying proportion of success.</p>
<p>Let’s say the zombie apocalypse is upon us and we have come up with a new experimental drug to cure zombieism. We have no clue how effective it’s going to be, but when we gave it to 13 zombies two of them turned human again. Change the data argument to <code>prop_model</code> to estimate the underlying proportion of success of curing a zombie.</p>
<pre class="r"><code>data &lt;- c(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
prop_model(data)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-32-1.png" width="672" />
The model implemented in prop_model makes more sense here as we had no clue how good the drug would be. The final probability distribution (at n=13) represents what the model now knows about the underlying proportion of cured zombies. What proportion of zombies would we expect to turn human if we administered this new drug to the whole zombie population?</p>
<p>Answer: Between 5% to 40%.</p>
<div id="priors-and-posteriors-1" class="section level4">
<h4>Priors and Posteriors</h4>
<p>Here again is the <code>prop_model</code> function which has been given the data from our zombie experiment where two out of 13 zombies got cured. In addition to producing a plot, <code>prop_model</code> also returns a large random sample from the posterior over the underlying proportion of success.</p>
<p>Assign the return value of <code>prop_model</code> to a variable called <code>posterior</code> and take a look at the first number of samples using the command <code>head(posterior)</code></p>
<pre class="r"><code>data = c(1, 0, 0, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0)
posterior &lt;- prop_model(data)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>head(posterior)</code></pre>
<pre><code>## [1] 0.28618601 0.52663955 0.29492983 0.03172872 0.12646159 0.22545022</code></pre>
<p>Looking at these first few samples confirms what is already shown in the plot: That the underlying proportion of cured zombies is likely somewhere between 5% and 50%. But these were just the first six samples in posterior which currently contain 10,000 samples (the default of <code>prop_model</code>).</p>
<p>Take a look at the distribution of all the samples in posterior by plotting it as a histogram using the <code>hist()</code>function with posterior as the first argument.</p>
<pre class="r"><code>hist(posterior, breaks = 30, xlim = c(0,1), col = &quot;palegreen4&quot;)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-stats-html-windows/bayesian-data-analysis-notes_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Compare this histogram to the plot produced directly by prop_model. You should notice that the histogram and the posterior distribution (at n=13) describe the same distribution.</p>
</div>
<div id="summarizing-the-drug-experiment-1" class="section level4">
<h4>Summarizing the Drug Experiment</h4>
<p>A point estimate is a single number used to summarize what’s known about a parameter of interest. It can be seen as a “best guess” of the value of the parameter. A commonly used point estimate is the median of the posterior. It’s the midpoint of the distribution, and it’s equally probable for the parameter value to be larger than the median as it is to be smaller than it.</p>
<pre class="r"><code>median(posterior)</code></pre>
<pre><code>## [1] 0.1857518</code></pre>
<p>So, a best guess is that the drug would cure around 18% of all zombies. Another common summary is to report an interval that includes the parameter of interest with a certain probability. This is called a <em>credible interval</em> (CI). With a posterior represented as a vector of samples you can calculate a CI using the <code>quantile()</code> function. Let’s calculate the 90% credible interval of <code>posterior</code></p>
<pre class="r"><code>quantile(posterior,c(.05,.95))</code></pre>
<pre><code>##         5%        95% 
## 0.05990058 0.38679638</code></pre>
<pre class="r"><code>quantile(posterior,c(.05,.95))</code></pre>
<pre><code>##         5%        95% 
## 0.05990058 0.38679638</code></pre>
</div>
</div>
</div>
</div>
