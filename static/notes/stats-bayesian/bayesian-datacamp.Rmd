---
title: "DataCamp Bayesian Data Analysis Course"
subtitle: ""
author:  "Gerard Torrats-Espinosa"
date: "Fall 2020"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
---

<style type="text/css">

h1.title { /* Title */
    font-size: 24px;
    color: DarkBlue;
    font-weight: bold;
    font-style: normal;
}
h3.subtitle { /* Subtitle */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.author { /* Author */
    font-size: 16px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}
h4.date { /* Date */
    font-size: 14px;
    color: Black;
    font-weight: normal;
    font-style: italic;
}


h1 { /* Header 1 */
    font-size: 22px;
    color: DarkBlue;
    font-weight: bold; <!-- this could be:  font-weight: bold -->
    font-style: normal;
}
h2 { /* Header 2 */
    font-size: 18px;
    color: DarkBlue;
    font-weight: bold;
    font-style: normal;
}
h3 { /* Header 3 */
    font-size: 16px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4 { /* Header 4 */
    font-size: 14px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h5 { /* Header 5 */
    font-size: 14px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}


body{ /* Normal  */
      font-size: 14px;
       color: Black;
      font-family: "Arial", Times, serif;
  }
td {  /* Table  */
  font-size: 8px;
}


code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 1

## A Model for a Zombie Drug
The role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update probability distributions to reflect what has ben learned from data.


A model for the proportion of success (success can be clicking and add, or curing a patient):

- The data is a vector of successes and failures represented by 0 an 1.
- There is an unknown underlying *proportion of success*.
- If a data point is a success, this is only a result of this underlying proportion.
- Prior to seeing any data, any underlying proportion of success is equally likely.
- The result is a probability distribution that represents what the model knows about the underlying proportion of success.

The model, `prop_model`, can be written in R as follows (this was written by DataCamp): 

```{r}

library(tidyverse)
library(ggjoy)
prop_model <- function (data = c(), prior_prop = c(1, 1), n_draws = 10000, 
    show_plot = TRUE) 
{
    data <- as.logical(data)
    proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
    data_indices <- round(seq(0, length(data), length.out = min(length(data) + 
        1, 20)))
    post_curves <- map_dfr(data_indices, function(i) {
        value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", 
            "Failure"))
        label <- paste0("n=", i)
        probability <- dbeta(proportion_success, prior_prop[1] + 
            sum(data[seq_len(i)]), prior_prop[2] + sum(!data[seq_len(i)]))
        probability <- probability/max(probability)
        tibble(value, label, proportion_success, probability)
    })
    post_curves$label <- fct_rev(factor(post_curves$label, levels = paste0("n=", 
        data_indices)))
    post_curves$value <- factor(post_curves$value, levels = c("Prior", 
        "Success", "Failure"))
    p <- ggplot(post_curves, aes(x = proportion_success, y = label, 
        height = probability, fill = value)) + geom_joy(stat = "identity", 
        color = "white", alpha = 0.8, panel_scaling = TRUE, size = 1) + 
        scale_y_discrete("", expand = c(0.01, 0)) + scale_x_continuous("Underlying proportion of success") + 
        scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), 
            name = "", drop = FALSE, labels = c("Prior   ", "Success   ", 
                "Failure   ")) + theme_light(base_size = 18) + 
        theme(legend.position = "top")
    if (show_plot) {
        print(p)
    }
    invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + 
        sum(!data)))
}
```

Now let's try and see. Assume you just flipped a coin four times and the result was heads, tails, tails, heads. If you code heads as a success and tails as a failure then the following R codes runs prop_model with this data

```{r}
data <- c(1, 0, 0, 1)
prop_model(data)
```

The output of `prop_model` is a plot showing what the model learns about the underlying proportion of success from each data point in the order you entered them. At n=0 there is no data, and all the model knows is that it's equally probable that the proportion of success is anything from 0% to 100%. At n=4 all data has been added, and the model knows a little bit more.

If we really were interested in the underlying proportion of heads of this coin then `prop_model` isn't particularly useful. Since it assumes that any underlying proportion of success is equally likely prior to seeing any data it will take a lot of coin flipping to convince `prop_model` that the coin is fair. This model is more appropriate in a situation where we have little background knowledge about the underlying proportion of success.


Let's say the zombie apocalypse is upon us and we have come up with a new experimental drug to cure zombieism. We have no clue how effective it's going to be, but when we gave it to 13 zombies two of them turned human again. Change the data argument to `prop_model` to estimate the underlying proportion of success of curing a zombie.


```{r}
data <- c(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
prop_model(data)
```
The model implemented in prop_model makes more sense here as we had no clue how good the drug would be. The final probability distribution (at n=13) represents what the model now knows about the underlying proportion of cured zombies. What proportion of zombies would we expect to turn human if we administered this new drug to the whole zombie population?

Answer: Between 5% to 40%.

### Priors and Posteriors

Here again is the `prop_model` function which has been given the data from our zombie experiment where two out of 13 zombies got cured. In addition to producing a plot, `prop_model` also returns a large random sample from the posterior over the underlying proportion of success.

Assign the return value of `prop_model` to a variable called `posterior` and take a look at the first number of samples using the command `head(posterior)`


```{r}
data = c(1, 0, 0, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0)
posterior <- prop_model(data)
head(posterior)
```

Looking at these first few samples confirms what is already shown in the plot: That the underlying proportion of cured zombies is likely somewhere between 5% and 50%. But these were just the first six samples in posterior which currently contain 10,000 samples (the default of `prop_model`).

Take a look at the distribution of all the samples in posterior by plotting it as a histogram using the `hist()`function with posterior as the first argument.

```{r}
hist(posterior, breaks = 30, xlim = c(0,1), col = "palegreen4")
```

Compare this histogram to the plot produced directly by prop_model. You should notice that the histogram and the posterior distribution (at n=13) describe the same distribution.

### Summarizing the Drug Experiment

A point estimate is a single number used to summarize what's known about a parameter of interest. It can be seen as a "best guess" of the value of the parameter. A commonly used point estimate is the median of the posterior. It's the midpoint of the distribution, and it's equally probable for the parameter value to be larger than the median as it is to be smaller than it.

```{r}
median(posterior)
```
So, a best guess is that the drug would cure around 18% of all zombies. Another common summary is to report an interval that includes the parameter of interest with a certain probability. This is called a *credible interval* (CI). With a posterior represented as a vector of samples you can calculate a CI using the `quantile()` function. Let's calculate the 90% credible interval of `posterior`

```{r}
quantile(posterior,c(.05,.95))
```

According to the credible interval, there is a 90% probability that the proportion of zombies the drug would cure is between 6% and 38%. (Here we have to be careful to remember that the percentage of cured zombies and the percentage of probability are two different things.)

Now, there is a rival zombie laboratory that is also working on a drug. They claim that they are certain that their drug cures 7% of the zombies it's administered to. Can we calculate how probable it is that our drug is better? Yes, we can! But it's a two stage process.

```{r}
sum(posterior>0.07)
```

The point of working with samples from a probability distribution is that it makes it easy to calculate new measures of interest. To turn this count into a probability we now need to normalize it, that is, divide it by the total number of samples in posterior.

```{r}
sum(posterior>0.07)/length(posterior)
```
We could summarize the zombie experiment as follows: "Given the data of two cured and 11 relapsed zombies, and using the Bayesian model described before, there is a 90% probability that our drug cures between 6% and 39% of treated zombies. Further, there is 93% probability that our drug cures zombies at a higher rate than current state of the art drugs."

This was a very simple Bayesian model, there was just one parameter to estimate, the underlying proportion of success, but it uses the exact same principles and machinery as more sophisticated models.