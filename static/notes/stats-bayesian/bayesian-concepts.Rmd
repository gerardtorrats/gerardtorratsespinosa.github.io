---
title: "Introduction to Bayesian Statistics^[These notes are summarized from Reich and Ghosh's book *Bayesian Statitical Methods*.]"
subtitle: ""
author:  "Gerard Torrats-Espinosa"
date: "Fall 2020"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
---

<style type="text/css">

h1.title { /* Title */
    font-size: 24px;
    color: DarkBlue;
    font-weight: bold;
    font-style: normal;
}
h3.subtitle { /* Subtitle */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.author { /* Author */
    font-size: 16px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}
h4.date { /* Date */
    font-size: 14px;
    color: Black;
    font-weight: normal;
    font-style: italic;
}


h1 { /* Header 1 */
    font-size: 22px;
    color: DarkBlue;
    font-weight: bold; <!-- this could be:  font-weight: bold -->
    font-style: normal;
}
h2 { /* Header 2 */
    font-size: 18px;
    color: DarkBlue;
    font-weight: bold;
    font-style: normal;
}
h3 { /* Header 3 */
    font-size: 16px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4 { /* Header 4 */
    font-size: 14px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h5 { /* Header 5 */
    font-size: 14px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}


body{ /* Normal  */
      font-size: 14px;
       color: Black;
      font-family: "Arial", Times, serif;
  }
td {  /* Table  */
  font-size: 8px;
}


code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Math Refresher {---}

## Working With Expectations {---}

## The Summmation Operator {---}

## Integrals and Distributions {---}


# Basics of Bayesian Inference

- [DS] Bayesian inference estimates unobservable or unknown quantities (eg, parameters of a model, or predictions about future data) given the data that we see by using probability to describe the uncertainty over what the values of the unknown quantities could be.
- [DS] One if its advantages is that it's a flexible method that can allow us to build problem-specific models.
- [DS] Probability is a statement about certainty / uncertainty
- 

## Probability Background

Difference between probability and statistics:

- Probability: Assumes a mathematical model that represents the process of interest and uses that model to compute the likelihood that some events happen (e.g., what is the probability of observing five heads in a row flipping a fair coin?).
- Statistics: Uses data to refine the probability model and test hypothesis related to the underlying process that generated the data (e.g., given that we observed five heads in a row, can we conclude that the coin is biased?).

### Univariate Distributions

Notation:

-  $X$ is the random variable that takes specific values $x$.
- $S$ is the support of $X$.

#### Discrete Distributions

For a discrete random variable $X$, the **probability mass function (PMF)** $f(x)$ assigns a probability to each element of $X$'s support:
$$Prob(X=x) = f(x)$$

The probability of the even that $X$ falls in a set $S' \subset S$ is the sum over elements in $S'$.

$$Prob(X \in S') = \sum_{x \in S'}  f(x) $$

This helps us define the **cumulative density function (CDF)**:

$$F(x) = Prob(X \leq x) = \sum_{c \leq x} f(c) $$

A PMF is a function from the support of X to the probability of events. 

It is useful to summarize the function using a few interpretable quantities such as the mean and the variance.

The **expected value or mean** is:

$$ E(X) = \sum_{x \in S} x f(x)$$
 
The **variance** measures the spread around the mean via the expected suqred deviation from the center of the distribution:

$$Var(X) = E \{ |X-E(X)^2| \} = \sum_{x \in S} [x - E(X)]^2 f(x)$$


