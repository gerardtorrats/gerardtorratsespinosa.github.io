---
title: "Examples of Bayesian Applications"
subtitle: ""
author:  "Gerard Torrats-Espinosa"
date: "Fall 2020"
output:
  html_document:
    number_sections: false
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">

h1.title { /* Title */
    font-size: 24px;
    color: DarkBlue;
    font-weight: bold;
    font-style: normal;
}
h3.subtitle { /* Subtitle */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.author { /* Author */
    font-size: 16px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}
h4.date { /* Date */
    font-size: 14px;
    color: Black;
    font-weight: normal;
    font-style: italic;
}


h1 { /* Header 1 */
    font-size: 22px;
    color: DarkBlue;
    font-weight: bold; <!-- this could be:  font-weight: bold -->
    font-style: normal;
}
h2 { /* Header 2 */
    font-size: 18px;
    color: DarkBlue;
    font-weight: bold;
    font-style: normal;
}
h3 { /* Header 3 */
    font-size: 16px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4 { /* Header 4 */
    font-size: 14px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h5 { /* Header 5 */
    font-size: 14px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}


body{ /* Normal  */
      font-size: 14px;
       color: Black;
      font-family: "Arial", Times, serif;
  }



blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    color: DarkBlue;
    border-left: 5px solid #eee;
}


td {  /* Table  */
  font-size: 8px;
}


code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>
<div id="theory-and-concepts" class="section level1">
<h1>Theory and Concepts<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h1>
</div>
<div id="basics-of-bayesian-inference" class="section level1">
<h1>Basics of Bayesian Inference</h1>
<div id="probability-background" class="section level2">
<h2>Probability Background</h2>
<p>Difference between probability and statistics:</p>
<ul>
<li>Probability: Assumes a mathematical model that represents the process of interest and uses that model to compute the likelihood that some events happen (e.g., what is the probability of observing five heads in a row flipping a fair coin?).</li>
<li>Statistics: Uses data to refine the probability model and test hypothesis related to the underlying process that generated the data (e.g., given that we observed five heads in a row, can we conclude that the coin is biased?).</li>
</ul>
<div id="univariate-distributions" class="section level3">
<h3>Univariate Distributions</h3>
<p>Notation:</p>
<ul>
<li><span class="math inline">\(X\)</span> is the random variable that takes specific values <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(S\)</span> is the support of <span class="math inline">\(X\)</span>.</li>
</ul>
<div id="discrete-distributions" class="section level4">
<h4>Discrete Distributions</h4>
<p>For a discrete random variable <span class="math inline">\(X\)</span>, the probability mass function (PMF) <span class="math inline">\(f(x)\)</span> assigns a probability to each element of <span class="math inline">\(X\)</span>’s support:
<span class="math display">\[Prob(X=x) = f(x)\]</span></p>
<p>The probability of the even that <span class="math inline">\(X\)</span> falls in a set <span class="math inline">\(S&#39; \subset S\)</span> is the sum over elements in <span class="math inline">\(S&#39;\)</span>.</p>
<p><span class="math display">\[Prob(X \in S&#39;) = \sum_{x \in S&#39;}  f(x) \]</span></p>
<p>This helps us define the cumulative density function (CDF):</p>
<p><span class="math display">\[F(x) = Prob(X \leq x) = \sum_{c \leq x} f(c) \]</span></p>
<p>A PMF is a function from the support of X to the probability of events.</p>
<p>It is useful to summarize the function using a few interpretable quantities such as the mean and the variance.</p>
<p>The expected value or mean is:</p>
<p><span class="math display">\[ E(X) = \sum_{x \in S} x f(x)\]</span></p>
<p>The variance measures the spread around the mean via the expected suqred deviation from the center of the distribution:</p>
<p><span class="math display">\[Var(X) = E \{ |X-E(X)^2| \} = \sum_{x \in S} [x - E(X)]^2 f(x)\]</span></p>
</div>
</div>
</div>
</div>
<div id="applications" class="section level1">
<h1>Applications<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></h1>
<div id="example-1-bayes-rule-and-covid-19-testing" class="section level2">
<h2>Example 1: Bayes Rule and COVID-19 Testing</h2>
<div id="taking-one-covid-19-test" class="section level3">
<h3>Taking One COVID-19 Test</h3>
<p>You take a COVID-19 test. What is the probability that you actually have COVID-19 if test comes out positive? Let’s start with some notation:</p>
<ul>
<li>Let <span class="math inline">\(\theta\)</span> represent your true state: <span class="math inline">\(\theta = 1\)</span> if you are infected with the virus and <span class="math inline">\(\theta = 0\)</span> if you are not.</li>
<li>Let <span class="math inline">\(y\)</span> denote the result of the test: <span class="math inline">\(y = 1\)</span> if the test comes back positive and <span class="math inline">\(y = 0\)</span> if the test is negative.</li>
<li>Define the probability of a correct result for an infected subject as <span class="math inline">\(p = Pr(y=1|\theta=1)\)</span>. We call this the <em>sensitivity</em> of the test.</li>
<li>Define the probability of a correct result for a non-infected subject as <span class="math inline">\(q = Pr(y=0|\theta=0)\)</span>. We call this the <em>specificity</em> of the test.</li>
<li>If we know the specificity, we know the <em>false positive probability</em>, <span class="math inline">\(q_0 = 1 - q = q = Pr(y=1|\theta=0)\)</span>.</li>
<li>Define the prior probability of being infected as <span class="math inline">\(\pi\)</span>. This is the marginal probability <span class="math inline">\(Pr(\theta = 1) = \pi\)</span> . We call this the <em>prevalence</em> of the outcome (in this case the prevalence of COVID-19).</li>
</ul>
<p>We know the following about COVID-19 and the test we took:</p>
<ul>
<li>We know that when we took the test, 4% of the population had COVID-19.</li>
<li>The nurse who helped us with the test told us that the test had a sensitivity of .90 and specificity of .92. This means that the false positive rate is .08.</li>
</ul>
<p>We set the following values in R:</p>
<pre class="r"><code>pi &lt;- .04  # Prior probability of infection (ie, Prevalence)
q1 &lt;- .90  # Sensitivity
q0 &lt;- .08  # False positive</code></pre>
<p>With this information we can compute the posterior probabilities, given test result.</p>
<p>Using Bayes Rule, we find that the posterior probability of infection given a positive test is:
<span class="math display">\[Pr(\theta = 1 |y_1 =1) = \frac{Pr(\theta = 1, y_1 = 1)}{Pr(y_1=1)} = \frac{Pr(\theta=1)Pr(y_1=1|\theta=1)}{Pr(y_1=1)} = \frac{\pi p }{\pi p + (1-\pi)q}\]</span></p>
<p>In R:</p>
<pre class="r"><code>pi * q1 / (pi*q1 + (1-pi)*q0)  # Probability of infection given a positive test result</code></pre>
<pre><code>## [1] 0.3191489</code></pre>
<p>Using Bayes Rule, we can also compute the posterior probability of infection had the test come out negative:
<span class="math display">\[Pr(\theta = 1 |y_1 =0) = \frac{Pr(\theta = 1, y_1 = 1)}{Pr(y_1=0)} = \frac{Pr(\theta=1)Pr(y_1=0|\theta=1)}{Pr(y_1=0)} = \frac{\pi (1-p) }{\pi (1-p) + (1-\pi)(1-q)}\]</span></p>
<p>In R:</p>
<pre class="r"><code>pi*(1-q1) / (pi*(1-q1) + (1-pi)*(1-q0))  # Probability of infection given a negative test result</code></pre>
<pre><code>## [1] 0.004508566</code></pre>
<div id="how-does-the-prior-influence-the-posterior" class="section level4">
<h4>How does the prior influence the posterior?</h4>
<p>One thing we see from looking at the expressions for the posterior probabilities <span class="math inline">\(Pr(\theta = 1 |y_1 =0)\)</span> and <span class="math inline">\(Pr(\theta =1 |y_1 =0)\)</span> above is that they both depend on the prior probability of infection, <span class="math inline">\(\pi\)</span>. We can plot how both posterior probabilities change as <span class="math inline">\(\pi\)</span> increases</p>
<p>In R:</p>
<pre class="r"><code>post.prob.pos.test &lt;- function(pi, q0, q1)
{
  pi * q1 / (pi*q1 + (1-pi)*q0)
}


post.prob.neg.test &lt;- function(pi, q0, q1)
{
  pi*(1-q1) / (pi*(1-q1) + (1-pi)*(1-q0))
}


grid &lt;- seq(.01, .99, .01)


par(mfrow=c(1,2)) 

plot(grid, post.prob.pos.test(pi=grid, q0=q0, q1=q1), type=&quot;l&quot;, 
     xlab=&quot;Prior probability \n (COVID-19 prevalence)&quot;, ylab=&quot;Posterior probability&quot;, 
     main=&quot;Probabiliy of infection \n given positive test&quot;, ylim=c(0,1))

plot(grid, post.prob.neg.test(pi=grid, q0=q0, q1=q1), type=&quot;l&quot;, 
     xlab=&quot;Prior probability  \n (COVID-19 prevalence)&quot;, ylab=&quot;Posterior probability&quot;, 
     main=&quot;Probabiliy of infection \n given negative test&quot;, ylim=c(0,1))</code></pre>
<p><img src="static/notes/stats-bayesian/bayesian-examples_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="how-do-the-sensitivity-and-specificty-influence-the-posterior" class="section level4">
<h4>How do the sensitivity and specificty influence the posterior?</h4>
<p>We first plot how the sensitivity influence the posteriors:</p>
<pre class="r"><code>par(mfrow=c(1,2))

plot(grid, post.prob.pos.test(pi=pi, q1=grid, q0=q0), type=&quot;l&quot;, 
     xlab=&quot;Sensitivity of the test&quot;, ylab=&quot;Posterior probability&quot;, 
     main=&quot;Probabiliy of infection \n given positive test&quot;, ylim=c(0,1))

plot(grid, post.prob.neg.test(pi=pi, q1=grid, q0=q0), type=&quot;l&quot;,
     xlab=&quot;Sensitivity of the test&quot;, ylab=&quot;Posterior probability&quot;, 
     main=&quot;Probabiliy of infection \n given negative test&quot;, ylim=c(0,1))</code></pre>
<p><img src="static/notes/stats-bayesian/bayesian-examples_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>We now plot how the specificity influences the posteriors:</p>
<pre class="r"><code>par(mfrow=c(1,2))

plot(grid, post.prob.pos.test(pi=pi, q1=q1, q0=1-grid), type=&quot;l&quot;, 
     xlab=&quot;Specificity of the test&quot;, ylab=&quot;Posterior probability&quot;, 
     main=&quot;Probabiliy of infection \n given positive test&quot;, ylim=c(0,1))

plot(grid, post.prob.neg.test(pi=pi, q1=q1, q0=1-grid), type=&quot;l&quot;,
     xlab=&quot;Specificity of the test&quot;, ylab=&quot;Posterior probability&quot;, 
     main=&quot;Probabiliy of infection \n given negative test&quot;, ylim=c(0,1))</code></pre>
<p><img src="static/notes/stats-bayesian/bayesian-examples_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
<div id="taking-two-covid-19-tests" class="section level3">
<h3>Taking Two COVID-19 Tests</h3>
<p>In the scenario above, when you only took one test, we obtained that the probability of actually being infected given a positive test is .032 (given a prior of .04, a test sensitivity of .90, and a test specificity of .92). The natural thing to do is to get tested again and ensure that this is not a false positive.</p>
<p>Assuming the following:</p>
<ul>
<li>Our health status has not changed between tests</li>
<li>The two tests are independent.</li>
</ul>
<p>What is the probability that we are infected given that we have obtained two positive test results?</p>
</div>
<div id="monte-carlo-approximation" class="section level3">
<h3>Monte Carlo approximation</h3>
<p>Simulate the joint distribution of theta and Y:</p>
<pre class="r"><code>S &lt;- 1000;  theta &lt;- NULL;  y &lt;- NULL;

for(s in 1:S)
{
  theta[s] &lt;- rbinom(1, 1, pi)
  prob &lt;- ifelse(theta[s]==1, q1, q0)
  y[s] &lt;- rbinom(1, 1, prob)
}

table(theta, y) / S  # approximate joint distribution</code></pre>
<pre><code>##      y
## theta     0     1
##     0 0.890 0.072
##     1 0.005 0.033</code></pre>
<pre class="r"><code>mean(theta[y==1])  #  Monte Carlo approx to posterior prob</code></pre>
<pre><code>## [1] 0.3142857</code></pre>
<p>Of course there’s no real need for Monte Carlo, when the exact joint distribution is available by straightforward calculation:</p>
<pre class="r"><code>joint &lt;- matrix(c((1-pi)*(1-q0), pi*(1-q1), (1-pi)*q0, pi*q1), 2,2)

rownames(joint) &lt;- c(&quot;theta=0&quot;, &quot;theta=1&quot;)

colnames(joint) &lt;- c(&quot;y=0&quot;, &quot;y=1&quot;)

joint</code></pre>
<pre><code>##            y=0    y=1
## theta=0 0.8832 0.0768
## theta=1 0.0040 0.0360</code></pre>
<pre class="r"><code>joint[2,2] / sum(joint[,2])</code></pre>
<pre><code>## [1] 0.3191489</code></pre>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>These notes are summarized from Reich and Ghosh’s book <em>Bayesian Statitical Methods</em>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Some of this examples are inspired in Ronald Neath’s course.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
