---
title: "Bayesian Statistics"
subtitle: "Introduction"
author:  
date: "Fall 2020"
output:
  html_document:
    number_sections: false
    toc: false
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---



<style type="text/css">


h1.title { /* Title */
    font-size: 30px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h3.subtitle { /* Subtitle */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.author { /* Author */
    font-size: 20px;
    color: DarkBlue;
    font-weight: normal;
    font-style: normal;
}
h4.date { /* Date */
    font-size: 14px;
    color: DarkBlue;
    font-weight: normal;
    font-style: italic;
}


h1 { /* Header 1 */
    font-size: 20px;
    color: Blue;
    font-weight: normal; <!-- this could be:  font-weight: bold -->
    font-style: normal;
}
h2 { /* Header 2 */
    font-size: 18px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h3 { /* Header 3 */
    font-size: 16px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h4 { /* Header 4 */
    font-size: 14px;
    color: Blue;
    font-weight: normal;
    font-style: normal;
}
h5 { /* Header 5 */
    font-size: 12px;
    color: Black;
    font-weight: normal;
    font-style: normal;
}


body{ /* Normal  */
      font-size: 12px;
      font-family: "Verdana", Times, serif;
  }
td {  /* Table  */
  font-size: 8px;
}


code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>
<div id="examples-from-bayesian-stats-course-columbia-fall-2020" class="section level1">
<h1>Examples from Bayesian Stats Course (Columbia, Fall 2020)</h1>
<div id="example-1a-bayes-rule" class="section level2">
<h2>Example 1a: Bayes Rule</h2>
<p>You go to the doctor for a COVID-19 rapid test. Let <code>theta</code> represent your true state: <code>theta</code> = 1 if you are infected with the virus, and <code>theta</code> = 0 if you are not. Let <code>y</code> denote the result of the test: <code>y</code> = 1 if the test comes back positive and <code>y</code> = 0 if the test is negative. The probability of a correct result for an infected subject is called the sensitivity of the test (<code>Pr(y=1|theta=1)</code>), we will denote it <code>p</code> here. The probability of correct result for one not infected with the virus is called the specificity (<code>Pr(Y=0|theta=0)</code>), denoted as <code>q.</code></p>
<p>Do you have the virus? Of course you don’t know. In Bayesian statistics we describe our uncertainty about the world by assigning probabilities to various states. Suppose your prior probability
of being infected is <code>pi</code>. That is, the marginal probability <code>Pr(theta = 1) = pi</code> .</p>
<p>With this information we can define the following:</p>
<ul>
<li><code>pi</code> as the prior prob of infection (“prevalence”): <code>pi = Pr(theta=1)</code></li>
<li><code>q0</code> is the false positive probability: <code>q0 = Pr(Y=1|theta=0)</code></li>
<li><code>q1</code> is the true positive probability: <code>q1 = Pr(Y=1|theta=1)</code>. This is also called the sensitivity of the test; <code>1-q0</code> is the specificity of the test.</li>
</ul>
<p>We set the following values in R:</p>
<pre class="r"><code>pi &lt;- .04   # 20% of population is infected
q1 &lt;- .90  # test has sensitivity of .90
q0 &lt;- .08  # test has specificity of .92</code></pre>
<p>With this we can compute the posterior probabilities, given test result.</p>
<p>The probability of infection, before the test result is available is <code>pi</code>:</p>
<pre class="r"><code>pi  # Prob of infection, before test result is available </code></pre>
<pre><code>## [1] 0.04</code></pre>
<p>The probability of infection given a positive test is:
<span class="math display">\[Pr(\theta|y_1 =1) = \frac{Pr(\theta = 1, y_1 = 1)}{Pr(y_1=1)} = \frac{Pr(\theta=1)Pr(y_1=1|\theta=1)}{Pr(y_1=1)} = \frac{\pi p }{\pi p + (1-\pi)q}\]</span></p>
<pre class="r"><code>pi * q1 / (pi*q1 + (1-pi)*q0)  # Prob of infection given positive test result</code></pre>
<pre><code>## [1] 0.3191489</code></pre>
<p>The probability of infection given a negative test is:
<span class="math display">\[Pr(\theta|y_1 =0) = \frac{Pr(\theta = 1, y_1 = 1)}{Pr(y_1=0)} = \frac{Pr(\theta=1)Pr(y_1=0|\theta=1)}{Pr(y_1=0)} = \frac{\pi (1-p) }{\pi (1-p) + (1-\pi)(1-q)}\]</span></p>
<pre class="r"><code>pi*(1-q1) / (pi*(1-q1) + (1-pi)*(1-q0))  # Prob of infection if test negative</code></pre>
<pre><code>## [1] 0.004508566</code></pre>
<p>Plot posterior probability versus prior probability:</p>
<pre class="r"><code>post.prob &lt;- function(pi, q0, q1)
{
  pi * q1 / (pi*q1 + (1-pi)*q0)
}

grid &lt;- seq(.01, .99, .01)

plot(grid, post.prob(pi=grid, q0=q0, q1=q1), type=&quot;l&quot;, 
     xlab=&quot;Prior probability&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-statistics/bayesian-statistics-introduction_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>That’s how the posterior prob varies with prior prob</p>
<p>What about the sensitivity and specificity of the test?</p>
<pre class="r"><code>op &lt;- par(mfrow=c(1,2))

plot(grid, post.prob(pi=pi, q1=grid, q0=q0), type=&quot;l&quot;, 
     xlab=&quot;Sensitivity of test&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))

plot(grid, post.prob(pi=pi, q1=q1, q0=1-grid), type=&quot;l&quot;,
     xlab=&quot;Specificity of test&quot;, ylab=&quot;Posterior prob&quot;, 
     main=&quot;Prob of infection given positive test&quot;, ylim=c(0,1))</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-statistics/bayesian-statistics-introduction_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>par(op)</code></pre>
<div id="monte-carlo-approximation" class="section level3">
<h3>Monte Carlo approximation</h3>
<p>Simulate the joint distribution of theta and Y:</p>
<pre class="r"><code>S &lt;- 1000;  theta &lt;- NULL;  y &lt;- NULL;

for(s in 1:S)
{
  theta[s] &lt;- rbinom(1, 1, pi)
  prob &lt;- ifelse(theta[s]==1, q1, q0)
  y[s] &lt;- rbinom(1, 1, prob)
}

table(theta, y) / S  # approximate joint distribution</code></pre>
<pre><code>##      y
## theta     0     1
##     0 0.892 0.076
##     1 0.002 0.030</code></pre>
<pre class="r"><code>mean(theta[y==1])  #  Monte Carlo approx to posterior prob</code></pre>
<pre><code>## [1] 0.2830189</code></pre>
<p>Of course there’s no real need for Monte Carlo, when the exact joint distribution is available by straightforward calculation:</p>
<pre class="r"><code>joint &lt;- matrix(c((1-pi)*(1-q0), pi*(1-q1), (1-pi)*q0, pi*q1), 2,2)

rownames(joint) &lt;- c(&quot;theta=0&quot;, &quot;theta=1&quot;)

colnames(joint) &lt;- c(&quot;y=0&quot;, &quot;y=1&quot;)

joint</code></pre>
<pre><code>##            y=0    y=1
## theta=0 0.8832 0.0768
## theta=1 0.0040 0.0360</code></pre>
<pre class="r"><code>joint[2,2] / sum(joint[,2])</code></pre>
<pre><code>## [1] 0.3191489</code></pre>
</div>
</div>
<div id="example-1b-beta-binomial-model" class="section level2">
<h2>Example 1b: Beta-Binomial Model</h2>
<p>Create data:</p>
<pre class="r"><code>rm(list=ls())

n &lt;- 100;  y &lt;- 60;

theta &lt;- seq(0,1,.01)</code></pre>
<p>Hyperparameters:</p>
<pre class="r"><code>a          &lt;- c(1,.5,2,20)
b          &lt;- c(1,.5,2, 1)</code></pre>
<p>Calculations:</p>
<pre class="r"><code>post_mean  &lt;- (y+a)/(n+a+b)
post_var   &lt;- (y+a)*(n-y+b)/((n+a+b)^2 * (n+a+b+1))
post_sd    &lt;- sqrt(post_var)
post_PG5   &lt;- 1-pbeta(0.5,a+y,b+n-y)
pri_mean   &lt;- (a)/(a+b)
pri_var    &lt;- (a)*(b)/((a+b)*(a+b)*(a+b+1))
pri_sd     &lt;- sqrt(pri_var)
pri_PG5    &lt;- 1-pbeta(0.5,a,b)</code></pre>
<p>Summary plot of prior and posterior on same axes</p>
<pre class="r"><code>plot(theta,dbeta(theta,1,1),lwd=2,type=&quot;l&quot;,
     xlab=expression(theta), ylab=&quot;Density&quot;,
     ylim=c(0,10), cex.lab=1.5)  

legend(&quot;topleft&quot;, c(&quot;Prior&quot;,&quot;Posterior&quot;), col=1:2,
       lwd=2, inset=0.05, cex=1.25)

lines(theta, dbeta(theta,y+1,n-y+1), col=2, lwd=2)</code></pre>
<p><img src="gerardtorratsespinosa.github.io/static/notes/bayesian-statistics/bayesian-statistics-introduction_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Consider four different priors:</p>
<pre class="r"><code>op &lt;- par(mf
